window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "simba_plus", "modulename": "simba_plus", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.datasets", "modulename": "simba_plus.datasets", "kind": "module", "doc": "<p>Builtin Datasets.</p>\n"}, {"fullname": "simba_plus.decoders", "modulename": "simba_plus.decoders", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.decoders.register_prob_model", "modulename": "simba_plus.decoders", "qualname": "register_prob_model", "kind": "function", "doc": "<p>Register probabilistic model</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>prob_model\n    Data probabilistic model\ndecoder\n    Decoder type of the probabilistic model</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">prob_model</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">decoder</span><span class=\"p\">:</span> <span class=\"nb\">type</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.decoders.RelationalEdgeDistributionDecoder", "modulename": "simba_plus.decoders", "qualname": "RelationalEdgeDistributionDecoder", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing them to be nested in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will also have their\nparameters converted when you call <code>to()</code>, etc.</p>\n\n<div class=\"alert note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "simba_plus.decoders.RelationalEdgeDistributionDecoder.__init__", "modulename": "simba_plus.decoders", "qualname": "RelationalEdgeDistributionDecoder.__init__", "kind": "function", "doc": "<p>Initialize the decoder with shared projection matrix per relation type.\nSee <a href=\"https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/hgt_conv.html#HGTConv\">https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/hgt_conv.html#HGTConv</a></p>\n\n<p>Args:\n    data: HeteroData with node types\n    encoded_channels: Number of dimensions of latent vector that will be decoded\n    projected_channels: Number of dimensions of projected latent vector onto the relation-specific space\n    add_covariate: add covariate to cell node</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">torch_geometric</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">hetero_data</span><span class=\"o\">.</span><span class=\"n\">HeteroData</span>,</span><span class=\"param\">\t<span class=\"n\">encoded_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">projected_channels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">add_covariate</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cpu&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">project</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">edgetype_specific_bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">edgetype_specific_scale</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">edgetype_specific_std</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">positive_scale</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "simba_plus.decoders.RelationalEdgeDistributionDecoder.device", "modulename": "simba_plus.decoders", "qualname": "RelationalEdgeDistributionDecoder.device", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.decoders.RelationalEdgeDistributionDecoder.edgetype_specific_bias", "modulename": "simba_plus.decoders", "qualname": "RelationalEdgeDistributionDecoder.edgetype_specific_bias", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.decoders.RelationalEdgeDistributionDecoder.edgetype_specific_scale", "modulename": "simba_plus.decoders", "qualname": "RelationalEdgeDistributionDecoder.edgetype_specific_scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.decoders.RelationalEdgeDistributionDecoder.edgetype_specific_std", "modulename": "simba_plus.decoders", "qualname": "RelationalEdgeDistributionDecoder.edgetype_specific_std", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.decoders.RelationalEdgeDistributionDecoder.prob_dict", "modulename": "simba_plus.decoders", "qualname": "RelationalEdgeDistributionDecoder.prob_dict", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.decoders.RelationalEdgeDistributionDecoder.add_covariate", "modulename": "simba_plus.decoders", "qualname": "RelationalEdgeDistributionDecoder.add_covariate", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.decoders.RelationalEdgeDistributionDecoder.project", "modulename": "simba_plus.decoders", "qualname": "RelationalEdgeDistributionDecoder.project", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">src_z</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">dst_z</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">src_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dst_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.decoders.RelationalEdgeDistributionDecoder.forward", "modulename": "simba_plus.decoders", "qualname": "RelationalEdgeDistributionDecoder.forward", "kind": "function", "doc": "<p>Decodes the latent variable per edge type</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span>,</span><span class=\"param\">\t<span class=\"n\">z_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">edge_index_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">scale_dict</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bias_dict</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">std_dict</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributions</span><span class=\"o\">.</span><span class=\"n\">distribution</span><span class=\"o\">.</span><span class=\"n\">Distribution</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.encoders", "modulename": "simba_plus.encoders", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.encoders.TransEncoder", "modulename": "simba_plus.encoders", "qualname": "TransEncoder", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing them to be nested in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will also have their\nparameters converted when you call <code>to()</code>, etc.</p>\n\n<div class=\"alert note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "simba_plus.encoders.TransEncoder.__init__", "modulename": "simba_plus.encoders", "qualname": "TransEncoder.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">torch_geometric</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">hetero_data</span><span class=\"o\">.</span><span class=\"n\">HeteroData</span>,</span><span class=\"param\">\t<span class=\"n\">n_latent_dims</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"o\">*</span><span class=\"n\">args</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "simba_plus.encoders.TransEncoder.encode", "modulename": "simba_plus.encoders", "qualname": "TransEncoder.encode", "kind": "function", "doc": "<p>Runs the encoder and computes node-wise latent variables.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.encoders.TransEncoder.forward", "modulename": "simba_plus.encoders", "qualname": "TransEncoder.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"alert note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluate", "modulename": "simba_plus.evaluate", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.evaluate.collate", "modulename": "simba_plus.evaluate", "qualname": "collate", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">data</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluate.decode", "modulename": "simba_plus.evaluate", "qualname": "decode", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">simba_plus</span><span class=\"o\">.</span><span class=\"n\">model_prox</span><span class=\"o\">.</span><span class=\"n\">LightningProxModel</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">torch_geometric</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Data</span>,</span><span class=\"param\">\t<span class=\"n\">n_negative_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluate.get_gexp_metrics", "modulename": "simba_plus.evaluate", "qualname": "get_gexp_metrics", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">simba_plus</span><span class=\"o\">.</span><span class=\"n\">model_prox</span><span class=\"o\">.</span><span class=\"n\">LightningProxModel</span>,</span><span class=\"param\">\t<span class=\"n\">eval_data</span><span class=\"p\">:</span> <span class=\"n\">torch_geometric</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Data</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">torch_geometric</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Data</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_negative_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluate.get_accessibility_metrics", "modulename": "simba_plus.evaluate", "qualname": "get_accessibility_metrics", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">simba_plus</span><span class=\"o\">.</span><span class=\"n\">model_prox</span><span class=\"o\">.</span><span class=\"n\">LightningProxModel</span>,</span><span class=\"param\">\t<span class=\"n\">eval_data</span><span class=\"p\">:</span> <span class=\"n\">torch_geometric</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Data</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">torch_geometric</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Data</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_negative_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluate.eval", "modulename": "simba_plus.evaluate", "qualname": "eval", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;cpu&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">eval_split</span><span class=\"p\">:</span> <span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s1\">&#39;test&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;val&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;test&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_negative_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">index_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluate.add_argument", "modulename": "simba_plus.evaluate", "qualname": "add_argument", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">parser</span><span class=\"p\">:</span> <span class=\"n\">argparse</span><span class=\"o\">.</span><span class=\"n\">ArgumentParser</span></span><span class=\"return-annotation\">) -> <span class=\"n\">argparse</span><span class=\"o\">.</span><span class=\"n\">ArgumentParser</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluate.main", "modulename": "simba_plus.evaluate", "qualname": "main", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">args</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils", "modulename": "simba_plus.evaluation_utils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.evaluation_utils.record_and_plot_graph_stats", "modulename": "simba_plus.evaluation_utils", "qualname": "record_and_plot_graph_stats", "kind": "function", "doc": "<p>This is the so-called\nThis function records statistics for a cell-gene graph dataset and plots specified metrics.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>data: The data object or subgraph, typically in PyTorch Geometric format.\nIt should be a bidirectional graph\nExpected keys are:\n<ul>\n<li><code>edge_index</code> representing edges between cells and genes.</li>\n<li><code>edge_type</code> or other features that identify if nodes are cells or genes.</li>\n</ul></li>\n<li>dataset_name: A name identifier for the dataset (e.g., 'Train', 'Validation', or 'Full Dataset').</li>\n<li>bin_size: Size of the bins for neighbor count distributions in the histograms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">data</span>, </span><span class=\"param\"><span class=\"n\">dataset_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;Dataset&#39;</span>, </span><span class=\"param\"><span class=\"n\">bin_size</span><span class=\"o\">=</span><span class=\"mi\">5</span>, </span><span class=\"param\"><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.record_and_plot_multiome_graph_stats", "modulename": "simba_plus.evaluation_utils", "qualname": "record_and_plot_multiome_graph_stats", "kind": "function", "doc": "<p>Records statistics for a heterogeneous graph dataset and plots specified metrics for multiple relationships,\nfollowing the logic of counting the occurrences of target nodes in edge indices.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>data: The data object or subgraph, typically in PyTorch Geometric format.\nExpected to be a bidirectional graph.</li>\n<li>relationships: List of tuples representing the relationships to process.\nEach tuple is (source_node_type, edge_type, target_node_type).</li>\n<li>dataset_name: A name identifier for the dataset (e.g., 'Train', 'Validation', or 'Full Dataset').</li>\n<li>bin_size: Size of the bins for neighbor count distributions in the histograms.</li>\n<li>save_path: Directory path to save the plots.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">data</span>,</span><span class=\"param\">\t<span class=\"n\">relationships</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;Dataset&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">bin_size</span><span class=\"o\">=</span><span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.summarize_epoch_stats", "modulename": "simba_plus.evaluation_utils", "qualname": "summarize_epoch_stats", "kind": "function", "doc": "<p>Summarizes the overall statistics of an epoch based on a list of batch DataFrames\nand plots the distribution of gene and cell neighbor counts.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>df_list: List of DataFrames where each DataFrame contains statistics for a batch,\nwith keys ['num_cells', 'num_genes', 'num_links', 'gene_neighbor_count', 'cell_neighbor_count'].</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>epoch_stats: A dictionary containing aggregated statistics for the epoch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">df_list</span>, </span><span class=\"param\"><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.scatter_color_var", "modulename": "simba_plus.evaluation_utils", "qualname": "scatter_color_var", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">y_pred_mean</span>,</span><span class=\"param\">\t<span class=\"n\">y_pred_var</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_scatter&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.plot_nll_distributions", "modulename": "simba_plus.evaluation_utils", "qualname": "plot_nll_distributions", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">class1</span>, </span><span class=\"param\"><span class=\"n\">class2</span>, </span><span class=\"param\"><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span>, </span><span class=\"param\"><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.empirical_coverage", "modulename": "simba_plus.evaluation_utils", "qualname": "empirical_coverage", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_values</span>,</span><span class=\"param\">\t<span class=\"n\">pred_means</span>,</span><span class=\"param\">\t<span class=\"n\">pred_vars</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_empirical_coverage&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.QQplot", "modulename": "simba_plus.evaluation_utils", "qualname": "QQplot", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_values</span>,</span><span class=\"param\">\t<span class=\"n\">pred_means</span>,</span><span class=\"param\">\t<span class=\"n\">pred_vars</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_empirical_coverage&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.densityMap", "modulename": "simba_plus.evaluation_utils", "qualname": "densityMap", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">y_pred_mean</span>,</span><span class=\"param\">\t<span class=\"n\">y_pred_var</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_empirical_coverage&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.Hist2DMap", "modulename": "simba_plus.evaluation_utils", "qualname": "Hist2DMap", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">y_pred_mean</span>,</span><span class=\"param\">\t<span class=\"n\">y_pred_var</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_empirical_coverage&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.HexbinMap", "modulename": "simba_plus.evaluation_utils", "qualname": "HexbinMap", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">y_pred_mean</span>,</span><span class=\"param\">\t<span class=\"n\">y_pred_var</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_empirical_coverage&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.ContourMap", "modulename": "simba_plus.evaluation_utils", "qualname": "ContourMap", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">y_pred_mean</span>,</span><span class=\"param\">\t<span class=\"n\">y_pred_var</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_empirical_coverage&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.CalibrationCurve", "modulename": "simba_plus.evaluation_utils", "qualname": "CalibrationCurve", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">probs</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_empirical_coverage&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.ROCCurve", "modulename": "simba_plus.evaluation_utils", "qualname": "ROCCurve", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">probs</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_empirical_coverage&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.PRCCurve", "modulename": "simba_plus.evaluation_utils", "qualname": "PRCCurve", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">probs</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_empirical_coverage&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.HistProbs", "modulename": "simba_plus.evaluation_utils", "qualname": "HistProbs", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">probs</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_empirical_coverage&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.BoxPlotProbs", "modulename": "simba_plus.evaluation_utils", "qualname": "BoxPlotProbs", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">probs</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_empirical_coverage&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.BetaDensityPlot", "modulename": "simba_plus.evaluation_utils", "qualname": "BetaDensityPlot", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">alpha_pred</span>,</span><span class=\"param\">\t<span class=\"n\">beta_pred</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_empirical_coverage&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.BetaCalibrationPlot", "modulename": "simba_plus.evaluation_utils", "qualname": "BetaCalibrationPlot", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">alpha_pred</span>,</span><span class=\"param\">\t<span class=\"n\">beta_pred</span>,</span><span class=\"param\">\t<span class=\"n\">fig_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_express_gene_empirical_coverage&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.compute_reconstruction_gene_metrics", "modulename": "simba_plus.evaluation_utils", "qualname": "compute_reconstruction_gene_metrics", "kind": "function", "doc": "<p>Computes reconstruction error, Pearson correlation, and Spearman correlation\nbetween original and reconstructed gene expression values.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>gene_expression: Tensor of original gene expression values (shape: [n_samples, n_genes]).</li>\n<li>recon_gene_expression: Tensor of reconstructed gene expression values (same shape as gene_expression).</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>metrics: A dictionary containing the reconstruction error, Pearson correlation, and Spearman correlation.</li>\n</ul>\n\n<h1 id=\"example-usage\">Example usage:</h1>\n\n<p># gene_expression = torch.rand(100, 500)  # Example original expression values\n  # recon_gene_expression = torch.rand(100, 500)  # Example reconstructed expression values\n  # metrics = compute_reconstruction_gene_metrics(gene_expression, recon_gene_expression)\n  # print(metrics)</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gene_expression</span>,</span><span class=\"param\">\t<span class=\"n\">recon_gene_expression</span>,</span><span class=\"param\">\t<span class=\"n\">recon_gene_std</span>,</span><span class=\"param\">\t<span class=\"n\">plot</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_exp_gene&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.compute_classification_metrics", "modulename": "simba_plus.evaluation_utils", "qualname": "compute_classification_metrics", "kind": "function", "doc": "<p>Computes binary classification metrics including cross-entropy loss, precision, recall,\naccuracy, and F1 score.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>targets: Tensor of true binary labels (shape: [n_samples]), with values 0 or 1.</li>\n<li>predictions: Tensor of predicted probabilities (shape: [n_samples]), with values between 0 and 1.</li>\n<li>threshold: Threshold to convert predicted probabilities into binary predictions.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>metrics: A dictionary containing cross-entropy loss, precision, recall, accuracy, and F1 score.</li>\n</ul>\n\n<h1 id=\"example-usage\">Example usage:</h1>\n\n<p># targets = torch.tensor([0, 1, 1, 0, 1])  # Example ground-truth binary labels\n  # predictions = torch.tensor([0.2, 0.8, 0.6, 0.4, 0.9])  # Example predicted probabilities\n  # metrics = compute_classification_metrics(targets, predictions, threshold=0.5)\n  # print(metrics)</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">targets</span>,</span><span class=\"param\">\t<span class=\"n\">predictions</span>,</span><span class=\"param\">\t<span class=\"n\">threshold</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">plot</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;cell_contains_peak&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.compute_asw_ari", "modulename": "simba_plus.evaluation_utils", "qualname": "compute_asw_ari", "kind": "function", "doc": "<p>Computes the Average Silhouette Width (ASW) and Adjusted Rand Index (ARI)\nfor the given embeddings and cell type labels.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>embeddings: Array-like or tensor of shape (n_samples, n_features), representing the model's cell embeddings.</li>\n<li>labels: Array-like or tensor of shape (n_samples,), representing the true cell type labels.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>metrics: A dictionary containing ASW and ARI scores.</li>\n</ul>\n\n<h1 id=\"example-usage\">Example usage:</h1>\n\n<p># embeddings = model_output  # Your cell embeddings (shape: [n_samples, n_features])\n  # labels = cell_type_labels  # Your true cell type labels (shape: [n_samples])\n  # metrics = compute_asw_ari(embeddings, labels)\n  # print(metrics)</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">embeddings</span>, </span><span class=\"param\"><span class=\"n\">labels</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.plot_sw_distribution", "modulename": "simba_plus.evaluation_utils", "qualname": "plot_sw_distribution", "kind": "function", "doc": "<p>Computes and plots the Silhouette Width (SW) score distributions for multiple models\nusing a violin plot.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>models_embeddings: List of arrays or tensors, where each element represents the embeddings\nfrom a model (shape for each: [n_samples, n_features]).</li>\n<li>labels: Array-like or tensor of shape (n_samples,), representing the true cell type labels.</li>\n<li>model_names: List of names (strings) corresponding to each model for labeling in the plot.</li>\n</ul>\n\n<h1 id=\"example-usage\">Example usage:</h1>\n\n<p># embeddings_model1 = model1_output  # Embeddings from model 1\n  # embeddings_model2 = model2_output  # Embeddings from model 2\n  # embeddings_model3 = model3_output  # Embeddings from model 3\n  # models_embeddings = [embeddings_model1, embeddings_model2, embeddings_model3]\n  # labels = cell_type_labels  # True cell type labels\n  # model_names = [\"Model 1\", \"Model 2\", \"Model 3\"]\n  # plot_sw_distribution(models_embeddings, labels, model_names)</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">models_embeddings</span>, </span><span class=\"param\"><span class=\"n\">labels</span>, </span><span class=\"param\"><span class=\"n\">model_names</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.evaluation_utils.compute_beta_metrics", "modulename": "simba_plus.evaluation_utils", "qualname": "compute_beta_metrics", "kind": "function", "doc": "<p>Compute evaluation metrics for a Beta distribution.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>y_true: Ground truth values (torch.Tensor, shape [N]).</li>\n<li>concentration1: Predicted concentration1 (alpha) values (torch.Tensor, shape [N]).</li>\n<li>concentration0: Predicted concentration0 (beta) values (torch.Tensor, shape [N]).</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>metrics: A dictionary containing MSE, Pearson r, Spearman r, and log-likelihood.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">y_true</span>,</span><span class=\"param\">\t<span class=\"n\">concentration1</span>,</span><span class=\"param\">\t<span class=\"n\">concentration0</span>,</span><span class=\"param\">\t<span class=\"n\">plot</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;gene_close_to_peak&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.get_adata", "modulename": "simba_plus.get_adata", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.get_adata.save_files", "modulename": "simba_plus.get_adata", "qualname": "save_files", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">prefix</span>, </span><span class=\"param\"><span class=\"n\">adata_CG_path</span>, </span><span class=\"param\"><span class=\"n\">adata_CP_path</span>, </span><span class=\"param\"><span class=\"n\">checkpoint_version_suffix</span><span class=\"o\">=</span><span class=\"s1\">&#39;&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.load_data", "modulename": "simba_plus.load_data", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.load_data.validate_input", "modulename": "simba_plus.load_data", "qualname": "validate_input", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">adata_CG</span>, </span><span class=\"param\"><span class=\"n\">adata_CP</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.load_data.type_attribute", "modulename": "simba_plus.load_data", "qualname": "type_attribute", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">data</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.load_data.make_sc_HetData", "modulename": "simba_plus.load_data", "qualname": "make_sc_HetData", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">adata_CG</span><span class=\"p\">:</span> <span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">_core</span><span class=\"o\">.</span><span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">AnnData</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">adata_CP</span><span class=\"p\">:</span> <span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">_core</span><span class=\"o\">.</span><span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">AnnData</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cell_cont_covariate_to_include</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Iterable</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cell_cat_cov</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.load_data.load_from_path", "modulename": "simba_plus.load_data", "qualname": "load_from_path", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cpu&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch_geometric</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">hetero_data</span><span class=\"o\">.</span><span class=\"n\">HeteroData</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.load_data.add_argument", "modulename": "simba_plus.load_data", "qualname": "add_argument", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">parser</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.load_data.main", "modulename": "simba_plus.load_data", "qualname": "main", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">args</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.loader", "modulename": "simba_plus.loader", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.loader.CustomIndexDataset", "modulename": "simba_plus.loader", "qualname": "CustomIndexDataset", "kind": "class", "doc": "<p>An abstract class representing a <code>Dataset</code>.</p>\n\n<p>All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite <code>__getitem__()</code>, supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite\n<code>__len__()</code>, which is expected to return the size of the dataset by many\n<code>~torch.utils.data.Sampler</code> implementations and the default options\nof <code>~torch.utils.data.DataLoader</code>. Subclasses could also\noptionally implement <code>__getitems__()</code>, for speedup batched samples\nloading. This method accepts list of indices of samples of batch and returns\nlist of samples.</p>\n\n<div class=\"alert note\">\n\n<p>sampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided.</p>\n\n</div>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "simba_plus.loader.CustomIndexDataset.__init__", "modulename": "simba_plus.loader", "qualname": "CustomIndexDataset.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">edge_attr_type</span>, </span><span class=\"param\"><span class=\"n\">index</span></span>)</span>"}, {"fullname": "simba_plus.loader.CustomIndexDataset.edge_attr_type", "modulename": "simba_plus.loader", "qualname": "CustomIndexDataset.edge_attr_type", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.loader.CustomIndexDataset.index", "modulename": "simba_plus.loader", "qualname": "CustomIndexDataset.index", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.losses", "modulename": "simba_plus.losses", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.losses.HSIC", "modulename": "simba_plus.losses", "qualname": "HSIC", "kind": "class", "doc": "<p>PyTorch implementation of Hilbert-Schmidt Independence Criterion (HSIC).</p>\n\n<p>HSIC measures the dependence between two random variables X and Y using kernel methods.\nThis implementation supports both linear and RBF kernels.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "simba_plus.losses.HSIC.__init__", "modulename": "simba_plus.losses", "qualname": "HSIC.__init__", "kind": "function", "doc": "<p>Initialize HSIC with kernel choices and parameters.</p>\n\n<p>Args:\n    kernel_x (str): Kernel type for X ('rbf' or 'linear')\n    kernel_y (str): Kernel type for Y ('rbf' or 'linear')\n    sigma_x (float, optional): Bandwidth for X's RBF kernel\n    sigma_y (float, optional): Bandwidth for Y's RBF kernel\n    use_cuda (bool): Whether to use GPU if available</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">kernel_x</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;rbf&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">kernel_y</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;rbf&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">sigma_x</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sigma_y</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_cuda</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">subset_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lam</span><span class=\"o\">=</span><span class=\"mf\">10000000.0</span></span>)</span>"}, {"fullname": "simba_plus.losses.HSIC.kernel_x", "modulename": "simba_plus.losses", "qualname": "HSIC.kernel_x", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.losses.HSIC.kernel_y", "modulename": "simba_plus.losses", "qualname": "HSIC.kernel_y", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.losses.HSIC.sigma_x", "modulename": "simba_plus.losses", "qualname": "HSIC.sigma_x", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.losses.HSIC.sigma_y", "modulename": "simba_plus.losses", "qualname": "HSIC.sigma_y", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.losses.HSIC.use_cuda", "modulename": "simba_plus.losses", "qualname": "HSIC.use_cuda", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.losses.HSIC.subset_samples", "modulename": "simba_plus.losses", "qualname": "HSIC.subset_samples", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.losses.HSIC.lam", "modulename": "simba_plus.losses", "qualname": "HSIC.lam", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.losses.HSIC.pair_loss", "modulename": "simba_plus.losses", "qualname": "HSIC.pair_loss", "kind": "function", "doc": "<p>Compute HSIC between X and Y.</p>\n\n<p>Args:\n    X (torch.Tensor): First variable (n x d1)\n    Y (torch.Tensor): Second variable (n x d2)\n    normalize (bool): Whether to compute normalized HSIC</p>\n\n<p>Returns:\n    Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:\n        If normalize=True: normalized HSIC\n        If normalize=False: (HSIC, HSIC(X,X), HSIC(Y,Y))</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.losses.HSIC.forward", "modulename": "simba_plus.losses", "qualname": "HSIC.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"alert note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">X</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.losses.HSIC.custom_train", "modulename": "simba_plus.losses", "qualname": "HSIC.custom_train", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">X</span>, </span><span class=\"param\"><span class=\"n\">optimizer</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.losses.weighted_mse_loss", "modulename": "simba_plus.losses", "qualname": "weighted_mse_loss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">weight</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.losses.bernoulli_kl_loss", "modulename": "simba_plus.losses", "qualname": "bernoulli_kl_loss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">p_logit</span>, </span><span class=\"param\"><span class=\"n\">q_logit</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox", "modulename": "simba_plus.model_prox", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel", "kind": "class", "doc": "<p>Hooks to be used in LightningModule.</p>\n", "bases": "lightning.pytorch.core.module.LightningModule"}, {"fullname": "simba_plus.model_prox.LightningProxModel.__init__", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.__init__", "kind": "function", "doc": "<p>Attributes:\n    prepare_data_per_node:\n        If True, each LOCAL_RANK=0 will call prepare data.\n        Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data.\n    allow_zero_length_dataloader_with_multiple_devices:\n        If True, dataloader with zero length within local rank is allowed.\n        Default value is False.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">torch_geometric</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">hetero_data</span><span class=\"o\">.</span><span class=\"n\">HeteroData</span>,</span><span class=\"param\">\tencoder_class: torch.nn.modules.module.Module = &lt;class &#x27;simba_plus.encoders.TransEncoder&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">n_hidden_dims</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>,</span><span class=\"param\">\t<span class=\"n\">n_latent_dims</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">50</span>,</span><span class=\"param\">\tdecoder_class: torch.nn.modules.module.Module = &lt;class &#x27;simba_plus.decoders.RelationalEdgeDistributionDecoder&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cpu&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_neg_samples_fold</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_layers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_heads</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">project_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">edgetype_specific_bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">edgetype_specific_scale</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">edgetype_specific_std</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">edge_types</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">hsic</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_no_kl</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">n_count_nodes</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">20</span>,</span><span class=\"param\">\t<span class=\"n\">n_kl_warmup</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">50</span>,</span><span class=\"param\">\t<span class=\"n\">nll_scale</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">val_nll_scale</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">node_weights_dict</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">nonneg</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">reweight_rarecell</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">reweight_rarecell_neighbors</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">positive_scale</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">train_data_dict</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">val_data_dict</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "simba_plus.model_prox.LightningProxModel.nonneg", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.nonneg", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.data", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.data", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.learning_rate", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.learning_rate", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.encoder", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.decoder", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.decoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.hsic", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.hsic", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.n_no_kl", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.n_no_kl", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.n_count_nodes", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.n_count_nodes", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.n_kl_warmup", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.n_kl_warmup", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.nll_scale", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.nll_scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.val_nll_scale", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.val_nll_scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.num_nodes_dict", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.num_nodes_dict", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.train_data_dict", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.train_data_dict", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.reweight_rarecell", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.reweight_rarecell", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.edgetype_loss_weight_dict", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.edgetype_loss_weight_dict", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.node_weights_dict", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.node_weights_dict", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.bias_dict", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.bias_dict", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.scale_dict", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.scale_dict", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.std_dict", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.std_dict", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.num_neg_samples_fold", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.num_neg_samples_fold", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.validation_step_outputs", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.validation_step_outputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.model_prox.LightningProxModel.on_train_start", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.on_train_start", "kind": "function", "doc": "<p>Called at the beginning of training after sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.encode", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.encode", "kind": "function", "doc": "<p>Runs the encoder and computes node-wise latent variables.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.reparametrize", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.reparametrize", "kind": "function", "doc": "<p>Generate random z from mu, logstd</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mu_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">logstd_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.project", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.project", "kind": "function", "doc": "<p>Projects all node in z_dict of edge_type to edge-type-specific space.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">torch_geometric</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">hetero_data</span><span class=\"o\">.</span><span class=\"n\">HeteroData</span>,</span><span class=\"param\">\t<span class=\"n\">z_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">edge_type</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.relational_recon_loss", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.relational_recon_loss", "kind": "function", "doc": "<p>Calculate reconstruction loss by maximizing log_prob of observing edge weight in pos_edge_index_dict and 0 weight in neg_edge_index_dict</p>\n\n<p>Args\nz_dict: encoded vector\npos_edge_index_dict: Dictionary of Tensors with shape (2, n_pos_edges)\npos_edge_weight_dict: Dictionary of Tensors with shape (n_pos_edges,) encoding the weight of each edges.\nneg_edge_index_dict: Dictionary of Tensors with shape (2, n_neg_edges) to be used as negative edges\nnum_neg_samples: If neg_edge_index is None and\n    num_neg_samples is None, This number of negative edges are sampled. Otherwise, the same number as the positive edges are sample.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch_geometric</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">hetero_data</span><span class=\"o\">.</span><span class=\"n\">HeteroData</span>,</span><span class=\"param\">\t<span class=\"n\">z_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">pos_edge_index_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">pos_edge_weight_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">neg_edge_index_dict</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">neg_sample</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">plot</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">get_metric</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.relational_kl_divergence", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.relational_kl_divergence", "kind": "function", "doc": "<p>Sums KL divergence across relations.</p>\n\n<p>Args\n    z_dict: encoded vector</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mu_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">logstd_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">node_index_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">node_weights_dict</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.kl_div_loss", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.kl_div_loss", "kind": "function", "doc": "<p>Args\nmu_dict: mu of encoded batch\nlogstd_dict: logstd of encoded batch\nnode_index_dict: node index of the batch\nnode_counts_dict: For entire dataset, counts how many times each node is used for KL div calculation. If None, assume no node has been used for the calculation.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mu_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">logstd_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">node_index_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">node_weights_dict</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">nodetype_loss_weight_dict</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.nll_loss", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.nll_loss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch_geometric</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">hetero_data</span><span class=\"o\">.</span><span class=\"n\">HeteroData</span>,</span><span class=\"param\">\t<span class=\"n\">z_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">pos_edge_index_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">pos_edge_weight_dict</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">neg_edge_index_dict</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">edgetype_loss_weight_dict</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">plot</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">get_metric</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.training_step", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.training_step", "kind": "function", "doc": "<p>Here you compute and return the training loss and some additional metrics for e.g. the progress bar or\nlogger.</p>\n\n<p>Args:\n    batch: The output of your data iterable, normally a <code>~torch.utils.data.DataLoader</code>.\n    batch_idx: The index of this batch.\n    dataloader_idx: The index of the dataloader that produced this batch.\n        (only if multiple dataloaders used)</p>\n\n<p>Return:\n    - <code>~torch.Tensor</code> - The loss tensor\n    - <code>dict</code> - A dictionary which can include any keys, but must include the key <code>'loss'</code> in the case of\n      automatic optimization.\n    - <code>None</code> - In automatic optimization, this will skip to the next batch (but is not supported for\n      multi-GPU, TPU, or DeepSpeed). For manual optimization, this has no special meaning, as returning\n      the loss is not required.</p>\n\n<p>In this step you'd normally do the forward pass and calculate the loss for a batch.\nYou can also do fancier things like multiple forward passes or something model specific.</p>\n\n<p>Example::</p>\n\n<pre><code>def training_step(self, batch, batch_idx):\n    x, y, z = batch\n    out = self.encoder(x)\n    loss = self.loss(out, x)\n    return loss\n</code></pre>\n\n<p>To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n    <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">automatic_optimization</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n\n\n<span class=\"c1\"># Multiple optimizers (e.g.: GANs)</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">training_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span><span class=\"p\">):</span>\n    <span class=\"n\">opt1</span><span class=\"p\">,</span> <span class=\"n\">opt2</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># do training_step with encoder</span>\n    <span class=\"o\">...</span>\n    <span class=\"n\">opt1</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n    <span class=\"c1\"># do training_step with decoder</span>\n    <span class=\"o\">...</span>\n    <span class=\"n\">opt2</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>Note:\n    When <code>accumulate_grad_batches</code> &gt; 1, the loss returned here will be automatically\n    normalized by <code>accumulate_grad_batches</code> internally.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.validation_step", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.validation_step", "kind": "function", "doc": "<p>Operates on a single batch of data from the validation set. In this step you'd might generate examples or\ncalculate anything of interest like accuracy.</p>\n\n<p>Args:\n    batch: The output of your data iterable, normally a <code>~torch.utils.data.DataLoader</code>.\n    batch_idx: The index of this batch.\n    dataloader_idx: The index of the dataloader that produced this batch.\n        (only if multiple dataloaders used)</p>\n\n<p>Return:\n    - <code>~torch.Tensor</code> - The loss tensor\n    - <code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.\n    - <code>None</code> - Skip to the next batch.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"c1\"># if you have one val dataloader:</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">validation_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span><span class=\"p\">):</span> <span class=\"o\">...</span>\n\n\n<span class=\"c1\"># if you have multiple val dataloaders:</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">validation_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span><span class=\"p\">,</span> <span class=\"n\">dataloader_idx</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span> <span class=\"o\">...</span>\n</code></pre>\n</div>\n\n<p>Examples::</p>\n\n<pre><code># CASE 1: A single validation dataset\ndef validation_step(self, batch, batch_idx):\n    x, y = batch\n\n    # implement your own\n    out = self(x)\n    loss = self.loss(out, y)\n\n    # log 6 example images\n    # or generated text... or whatever\n    sample_imgs = x[:6]\n    grid = torchvision.utils.make_grid(sample_imgs)\n    self.logger.experiment.add_image('example_images', grid, 0)\n\n    # calculate acc\n    labels_hat = torch.argmax(out, dim=1)\n    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n\n    # log the outputs!\n    self.log_dict({'val_loss': loss, 'val_acc': val_acc})\n</code></pre>\n\n<p>If you pass in multiple val dataloaders, <code>validation_step()</code> will have an additional argument. We recommend\nsetting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"c1\"># CASE 2: multiple validation dataloaders</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">validation_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span><span class=\"p\">,</span> <span class=\"n\">dataloader_idx</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span>\n    <span class=\"c1\"># dataloader_idx tells you which dataset this is.</span>\n    <span class=\"o\">...</span>\n</code></pre>\n</div>\n\n<p>Note:\n    If you don't need to validate you don't need to implement this method.</p>\n\n<p>Note:\n    When the <code>validation_step()</code> is called, the model has been put in eval mode\n    and PyTorch gradients have been disabled. At the end of validation,\n    the model goes back to training mode and gradients are enabled.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.mean_cell_neighbor_distance", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.mean_cell_neighbor_distance", "kind": "function", "doc": "<p>Calculates the mean distance between each cell node and its k nearest neighbors.\nReturns a tensor of mean distances for each cell.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">50</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.on_train_epoch_start", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.on_train_epoch_start", "kind": "function", "doc": "<p>Called in the training loop at the very beginning of the epoch.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.configure_optimizers", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.configure_optimizers", "kind": "function", "doc": "<p>Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.\nBut in the case of GANs or similar you might have multiple. Optimization with multiple optimizers only works in\nthe manual optimization mode.</p>\n\n<p>Return:\n    Any of these 6 options.</p>\n\n<pre><code>- **Single optimizer**.\n- **List or Tuple** of optimizers.\n- **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers\n  (or multiple ``lr_scheduler_config``).\n- **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"``\n  key whose value is a single LR scheduler or ``lr_scheduler_config``.\n- **None** - Fit will run without any optimizer.\n</code></pre>\n\n<p>The <code>lr_scheduler_config</code> is a dictionary which contains the scheduler and its associated configuration.\nThe default configuration is shown below.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">lr_scheduler_config</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"c1\"># REQUIRED: The scheduler instance</span>\n    <span class=\"s2\">&quot;scheduler&quot;</span><span class=\"p\">:</span> <span class=\"n\">lr_scheduler</span><span class=\"p\">,</span>\n    <span class=\"c1\"># The unit of the scheduler&#39;s step size, could also be &#39;step&#39;.</span>\n    <span class=\"c1\"># &#39;epoch&#39; updates the scheduler on epoch end whereas &#39;step&#39;</span>\n    <span class=\"c1\"># updates it after a optimizer update.</span>\n    <span class=\"s2\">&quot;interval&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;epoch&quot;</span><span class=\"p\">,</span>\n    <span class=\"c1\"># How many epochs/steps should pass between calls to</span>\n    <span class=\"c1\"># `scheduler.step()`. 1 corresponds to updating the learning</span>\n    <span class=\"c1\"># rate after every epoch/step.</span>\n    <span class=\"s2\">&quot;frequency&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"c1\"># Metric to monitor for schedulers like `ReduceLROnPlateau`</span>\n    <span class=\"s2\">&quot;monitor&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;val_loss&quot;</span><span class=\"p\">,</span>\n    <span class=\"c1\"># If set to `True`, will enforce that the value specified &#39;monitor&#39;</span>\n    <span class=\"c1\"># is available when the scheduler is updated, thus stopping</span>\n    <span class=\"c1\"># training if not found. If set to `False`, it will only produce a warning</span>\n    <span class=\"s2\">&quot;strict&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"c1\"># If using the `LearningRateMonitor` callback to monitor the</span>\n    <span class=\"c1\"># learning rate progress, this keyword can be used to specify</span>\n    <span class=\"c1\"># a custom logged name</span>\n    <span class=\"s2\">&quot;name&quot;</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<p>When there are schedulers in which the <code>.step()</code> method is conditioned on a value, such as the\n<code>torch.optim.lr_scheduler.ReduceLROnPlateau</code> scheduler, Lightning requires that the\n<code>lr_scheduler_config</code> contains the keyword <code>\"monitor\"</code> set to the metric name that the scheduler\nshould be conditioned on.</p>\n\n<p>.. testcode::</p>\n\n<pre><code># The ReduceLROnPlateau scheduler requires a monitor\ndef configure_optimizers(self):\n    optimizer = Adam(...)\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": ReduceLROnPlateau(optimizer, ...),\n            \"monitor\": \"metric_to_track\",\n            \"frequency\": \"indicates how often the metric is updated\",\n            # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\n            # multiple of \"trainer.check_val_every_n_epoch\".\n        },\n    }\n\n\n# In the case of two optimizers, only one using the ReduceLROnPlateau scheduler\ndef configure_optimizers(self):\n    optimizer1 = Adam(...)\n    optimizer2 = SGD(...)\n    scheduler1 = ReduceLROnPlateau(optimizer1, ...)\n    scheduler2 = LambdaLR(optimizer2, ...)\n    return (\n        {\n            \"optimizer\": optimizer1,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler1,\n                \"monitor\": \"metric_to_track\",\n            },\n        },\n        {\"optimizer\": optimizer2, \"lr_scheduler\": scheduler2},\n    )\n</code></pre>\n\n<p>Metrics can be made available to monitor by simply logging it using\n<code>self.log('metric_to_track', metric_val)</code> in your <code>~lightning.pytorch.core.LightningModule</code>.</p>\n\n<p>Note:\n    Some things to know:</p>\n\n<pre><code>- Lightning calls ``.backward()`` and ``.step()`` automatically in case of automatic optimization.\n- If a learning rate scheduler is specified in ``configure_optimizers()`` with key\n  ``\"interval\"`` (default \"epoch\") in the scheduler configuration, Lightning will call\n  the scheduler's ``.step()`` method automatically in case of automatic optimization.\n- If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizer.\n- If you use `torch.optim.LBFGS`, Lightning handles the closure function automatically for you.\n- If you use multiple optimizers, you will have to switch to 'manual optimization' mode and step them\n  yourself.\n- If you need to control how often the optimizer steps, override the `optimizer_step()` hook.\n</code></pre>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.lr_scheduler_step", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.lr_scheduler_step", "kind": "function", "doc": "<p>Override this method to adjust the default way the <code>~lightning.pytorch.trainer.trainer.Trainer</code> calls\neach scheduler. By default, Lightning calls <code>step()</code> and as shown in the example for each scheduler based on\nits <code>interval</code>.</p>\n\n<p>Args:\n    scheduler: Learning rate scheduler.\n    metric: Value of the monitor used for schedulers like <code>ReduceLROnPlateau</code>.</p>\n\n<p>Examples::</p>\n\n<pre><code># DEFAULT\ndef lr_scheduler_step(self, scheduler, metric):\n    if metric is None:\n        scheduler.step()\n    else:\n        scheduler.step(metric)\n\n# Alternative way to update schedulers if it requires an epoch value\ndef lr_scheduler_step(self, scheduler, metric):\n    scheduler.step(epoch=self.current_epoch)\n</code></pre>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">scheduler</span>, </span><span class=\"param\"><span class=\"n\">metric</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.model_prox.LightningProxModel.on_save_checkpoint", "modulename": "simba_plus.model_prox", "qualname": "LightningProxModel.on_save_checkpoint", "kind": "function", "doc": "<p>Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to\nsave.</p>\n\n<p>Args:\n    checkpoint: The full checkpoint dictionary before it gets dumped to a file.\n        Implementations of this hook can insert additional data into this dictionary.</p>\n\n<p>Example::</p>\n\n<pre><code>def on_save_checkpoint(self, checkpoint):\n    # 99% of use cases you don't need to implement this method\n    checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object\n</code></pre>\n\n<p>Note:\n    Lightning saves all aspects of training (epoch, global step, etc...)\n    including amp scaling.\n    There is no need for you to store anything about training.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">checkpoint</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.plotting", "modulename": "simba_plus.plotting", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.plotting.plot_heatmap", "modulename": "simba_plus.plotting", "qualname": "plot_heatmap", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">df_c</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">df_s</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.post_training", "modulename": "simba_plus.post_training", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.post_training.linking", "modulename": "simba_plus.post_training.linking", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.post_training.linking.get_corr_sign", "modulename": "simba_plus.post_training.linking", "qualname": "get_corr_sign", "kind": "function", "doc": "<p>Get correlation sign between gene and peak scores.\nArgs:\n                adata_G: AnnData object containing gene data.\n                adata_P: AnnData object containing peak data.\n                gidx: Indices of genes to consider.\n                pidx: Indices of peaks to consider.\nReturns:\n                DataFrame containing correlation signs between genes and peaks.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">adata_G</span>, </span><span class=\"param\"><span class=\"n\">adata_P</span>, </span><span class=\"param\"><span class=\"n\">gidx</span>, </span><span class=\"param\"><span class=\"n\">pidx</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.post_training.linking.ttest_group", "modulename": "simba_plus.post_training.linking", "qualname": "ttest_group", "kind": "function", "doc": "<p>Perform t-test for each group for val1 * val2 vs val1 * (permuted val2) and return p-values and log fold changes.\nArgs:\n        val1: First set of values.\n        val2: Second set of values.\n        group: Group labels for the values.\n        n_random_perm: Number of random permutations for significance testing.\nReturns:\n                Positive correlation scores, random scores, p-values, and log fold changes.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">val1</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">series</span><span class=\"o\">.</span><span class=\"n\">Series</span>,</span><span class=\"param\">\t<span class=\"n\">val2</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">series</span><span class=\"o\">.</span><span class=\"n\">Series</span>,</span><span class=\"param\">\t<span class=\"n\">group</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">series</span><span class=\"o\">.</span><span class=\"n\">Series</span>,</span><span class=\"param\">\t<span class=\"n\">n_random_perm</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">series</span><span class=\"o\">.</span><span class=\"n\">Series</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.post_training.linking.get_path_scores", "modulename": "simba_plus.post_training.linking", "qualname": "get_path_scores", "kind": "function", "doc": "<p>Get link scores between genes and peaks based on cell data.\nArgs:\n                    adata_C: AnnData object containing cell data.\n                    adata_G: AnnData object containing gene data.\n                    adata_P: AnnData object containing peak data.\n    Returns:\n                    DataFrame containing link scores between genes and peaks.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">adata_C</span><span class=\"p\">:</span> <span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">_core</span><span class=\"o\">.</span><span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">AnnData</span>,</span><span class=\"param\">\t<span class=\"n\">adata_G</span><span class=\"p\">:</span> <span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">_core</span><span class=\"o\">.</span><span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">AnnData</span>,</span><span class=\"param\">\t<span class=\"n\">adata_P</span><span class=\"p\">:</span> <span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">_core</span><span class=\"o\">.</span><span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">AnnData</span>,</span><span class=\"param\">\t<span class=\"n\">gene</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">peak</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">sign</span><span class=\"p\">:</span> <span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s1\">&#39;-&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;+&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;+&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">cell_idx</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_per_cell_score</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_gene_peak_score</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.post_training.linking.get_active_cell_state", "modulename": "simba_plus.post_training.linking", "qualname": "get_active_cell_state", "kind": "function", "doc": "<p>Get active cell state based on gene and peak scores.\nArgs:\n                adata_C: AnnData object containing cell data.\n                adata_G: AnnData object containing gene data.\n                adata_P: AnnData object containing peak data.\n                gene_idx: Indices of genes to consider.\n                peak_idx: Indices of peaks to consider.\n                sign: Sign of the scores to consider, either \"+\" or \"-\".\n                celltype_annot: Annotation for cell types.\n                return_p: Whether to return p-values.\nReturns:\n                Active cell state scores, random scores, significant cell types, and log fold changes.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">adata_C</span><span class=\"p\">:</span> <span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">_core</span><span class=\"o\">.</span><span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">AnnData</span>,</span><span class=\"param\">\t<span class=\"n\">adata_G</span><span class=\"p\">:</span> <span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">_core</span><span class=\"o\">.</span><span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">AnnData</span>,</span><span class=\"param\">\t<span class=\"n\">adata_P</span><span class=\"p\">:</span> <span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">_core</span><span class=\"o\">.</span><span class=\"n\">anndata</span><span class=\"o\">.</span><span class=\"n\">AnnData</span>,</span><span class=\"param\">\t<span class=\"n\">gene_idx</span>,</span><span class=\"param\">\t<span class=\"n\">peak_idx</span>,</span><span class=\"param\">\t<span class=\"n\">signs</span>,</span><span class=\"param\">\t<span class=\"n\">celltype_annot</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;azimuth_celltype&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">return_p</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">n_random_perm</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.prob_decoders", "modulename": "simba_plus.prob_decoders", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.prob_decoders.MAX_LOGSTD", "modulename": "simba_plus.prob_decoders", "qualname": "MAX_LOGSTD", "kind": "variable", "doc": "<p></p>\n", "default_value": "10"}, {"fullname": "simba_plus.prob_decoders.EPS", "modulename": "simba_plus.prob_decoders", "qualname": "EPS", "kind": "variable", "doc": "<p></p>\n", "default_value": "1e-07"}, {"fullname": "simba_plus.prob_decoders.ProximityDecoder", "modulename": "simba_plus.prob_decoders", "qualname": "ProximityDecoder", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing them to be nested in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will also have their\nparameters converted when you call <code>to()</code>, etc.</p>\n\n<div class=\"alert note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "simba_plus.prob_decoders.ProximityDecoder.__init__", "modulename": "simba_plus.prob_decoders", "qualname": "ProximityDecoder.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">()</span>"}, {"fullname": "simba_plus.prob_decoders.ProximityDecoder.forward", "modulename": "simba_plus.prob_decoders", "qualname": "ProximityDecoder.forward", "kind": "function", "doc": "<p>Args\nu: Input source tensor of shape (n_edges, n_latent_dimension)\nv: Input destination tensor of shape (n_edges, n_latent_dimension)\nlibrary_size: Library size of source node of shape (n_edges,)\nsrc_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ndst_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ncat_covs: Categorical covariates of source node of shape (n_edges, n_cat_covariates)</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">u</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">v</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.prob_decoders.NormalDataDecoder", "modulename": "simba_plus.prob_decoders", "qualname": "NormalDataDecoder", "kind": "class", "doc": "<p>Normal data decoder</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>out_features\n    Output dimensionality\nn_batches\n    Number of batches</p>\n", "bases": "ProximityDecoder"}, {"fullname": "simba_plus.prob_decoders.NormalDataDecoder.__init__", "modulename": "simba_plus.prob_decoders", "qualname": "NormalDataDecoder.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">out_features</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">n_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">positive_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "simba_plus.prob_decoders.NormalDataDecoder.positive_scale", "modulename": "simba_plus.prob_decoders", "qualname": "NormalDataDecoder.positive_scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.prob_decoders.NormalDataDecoder.forward", "modulename": "simba_plus.prob_decoders", "qualname": "NormalDataDecoder.forward", "kind": "function", "doc": "<p>Args\nu: Input source tensor of shape (n_edges, n_latent_dimension)\nv: Input destination tensor of shape (n_edges, n_latent_dimension)\nlibrary_size: Library size of source node of shape (n_edges,)\nsrc_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ndst_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ncat_covs: Categorical covariates of source node of shape (n_edges, n_cat_covariates)</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">u</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">v</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">src_size_factor</span>,</span><span class=\"param\">\t<span class=\"n\">dst_size_factor</span>,</span><span class=\"param\">\t<span class=\"n\">src_scale</span>,</span><span class=\"param\">\t<span class=\"n\">src_bias</span>,</span><span class=\"param\">\t<span class=\"n\">src_std</span>,</span><span class=\"param\">\t<span class=\"n\">dst_scale</span>,</span><span class=\"param\">\t<span class=\"n\">dst_bias</span>,</span><span class=\"param\">\t<span class=\"n\">dst_std</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributions</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"o\">.</span><span class=\"n\">Normal</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.prob_decoders.GammaDataDecoder", "modulename": "simba_plus.prob_decoders", "qualname": "GammaDataDecoder", "kind": "class", "doc": "<p>Gamma data decoder</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>out_features\n    Output dimensionality\nn_batches\n    Number of batches</p>\n", "bases": "ProximityDecoder"}, {"fullname": "simba_plus.prob_decoders.GammaDataDecoder.__init__", "modulename": "simba_plus.prob_decoders", "qualname": "GammaDataDecoder.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">out_features</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">n_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">positive_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "simba_plus.prob_decoders.GammaDataDecoder.positive_scale", "modulename": "simba_plus.prob_decoders", "qualname": "GammaDataDecoder.positive_scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.prob_decoders.GammaDataDecoder.forward", "modulename": "simba_plus.prob_decoders", "qualname": "GammaDataDecoder.forward", "kind": "function", "doc": "<p>Args\nu: Input source tensor of shape (n_edges, n_latent_dimension)\nv: Input destination tensor of shape (n_edges, n_latent_dimension)\nlibrary_size: Library size of source node of shape (n_edges,)\nsrc_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ndst_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ncat_covs: Categorical covariates of source node of shape (n_edges, n_cat_covariates)</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">u</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">v</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">src_size_factor</span>,</span><span class=\"param\">\t<span class=\"n\">dst_size_factor</span>,</span><span class=\"param\">\t<span class=\"n\">src_scale</span>,</span><span class=\"param\">\t<span class=\"n\">src_bias</span>,</span><span class=\"param\">\t<span class=\"n\">src_std</span>,</span><span class=\"param\">\t<span class=\"n\">dst_scale</span>,</span><span class=\"param\">\t<span class=\"n\">dst_bias</span>,</span><span class=\"param\">\t<span class=\"n\">dst_std</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributions</span><span class=\"o\">.</span><span class=\"n\">gamma</span><span class=\"o\">.</span><span class=\"n\">Gamma</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.prob_decoders.PoissonDataDecoder", "modulename": "simba_plus.prob_decoders", "qualname": "PoissonDataDecoder", "kind": "class", "doc": "<p>Normal data decoder</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>out_features\n    Output dimensionality\nn_batches\n    Number of batches</p>\n", "bases": "ProximityDecoder"}, {"fullname": "simba_plus.prob_decoders.PoissonDataDecoder.__init__", "modulename": "simba_plus.prob_decoders", "qualname": "PoissonDataDecoder.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">out_features</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">n_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span></span>)</span>"}, {"fullname": "simba_plus.prob_decoders.PoissonDataDecoder.forward", "modulename": "simba_plus.prob_decoders", "qualname": "PoissonDataDecoder.forward", "kind": "function", "doc": "<p>Args\nu: Input source tensor of shape (n_edges, n_latent_dimension)\nv: Input destination tensor of shape (n_edges, n_latent_dimension)\nlibrary_size: Library size of source node of shape (n_edges,)\nsrc_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ndst_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ncat_covs: Categorical covariates of source node of shape (n_edges, n_cat_covariates)</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">u</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">v</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">src_size_factor</span>,</span><span class=\"param\">\t<span class=\"n\">dst_size_factor</span>,</span><span class=\"param\">\t<span class=\"n\">src_scale</span>,</span><span class=\"param\">\t<span class=\"n\">src_bias</span>,</span><span class=\"param\">\t<span class=\"n\">src_std</span>,</span><span class=\"param\">\t<span class=\"n\">dst_scale</span>,</span><span class=\"param\">\t<span class=\"n\">dst_bias</span>,</span><span class=\"param\">\t<span class=\"n\">dst_std</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributions</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"o\">.</span><span class=\"n\">Normal</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.prob_decoders.BernoulliDataDecoder", "modulename": "simba_plus.prob_decoders", "qualname": "BernoulliDataDecoder", "kind": "class", "doc": "<p>Normal data decoder</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>out_features\n    Output dimensionality\nn_batches\n    Number of batches</p>\n", "bases": "ProximityDecoder"}, {"fullname": "simba_plus.prob_decoders.BernoulliDataDecoder.__init__", "modulename": "simba_plus.prob_decoders", "qualname": "BernoulliDataDecoder.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">out_features</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">n_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">positive_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "simba_plus.prob_decoders.BernoulliDataDecoder.positive_scale", "modulename": "simba_plus.prob_decoders", "qualname": "BernoulliDataDecoder.positive_scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.prob_decoders.BernoulliDataDecoder.forward", "modulename": "simba_plus.prob_decoders", "qualname": "BernoulliDataDecoder.forward", "kind": "function", "doc": "<p>Args\nu: Input source tensor of shape (n_edges, n_latent_dimension)\nv: Input destination tensor of shape (n_edges, n_latent_dimension)\nlibrary_size: Library size of source node of shape (n_edges,)\nsrc_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ndst_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ncat_covs: Categorical covariates of source node of shape (n_edges, n_cat_covariates)</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">u</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">v</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">src_size_factor</span>,</span><span class=\"param\">\t<span class=\"n\">dst_size_factor</span>,</span><span class=\"param\">\t<span class=\"n\">src_scale</span>,</span><span class=\"param\">\t<span class=\"n\">src_bias</span>,</span><span class=\"param\">\t<span class=\"n\">src_std</span>,</span><span class=\"param\">\t<span class=\"n\">dst_scale</span>,</span><span class=\"param\">\t<span class=\"n\">dst_bias</span>,</span><span class=\"param\">\t<span class=\"n\">dst_std</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributions</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"o\">.</span><span class=\"n\">Normal</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.prob_decoders.BetaDataDecoder", "modulename": "simba_plus.prob_decoders", "qualname": "BetaDataDecoder", "kind": "class", "doc": "<p>Normal data decoder</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>out_features\n    Output dimensionality\nn_batches\n    Number of batches</p>\n", "bases": "ProximityDecoder"}, {"fullname": "simba_plus.prob_decoders.BetaDataDecoder.__init__", "modulename": "simba_plus.prob_decoders", "qualname": "BetaDataDecoder.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">out_features</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">n_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span></span>)</span>"}, {"fullname": "simba_plus.prob_decoders.BetaDataDecoder.forward", "modulename": "simba_plus.prob_decoders", "qualname": "BetaDataDecoder.forward", "kind": "function", "doc": "<p>Args\nu: Input source tensor of shape (n_edges, n_latent_dimension)\nv: Input destination tensor of shape (n_edges, n_latent_dimension)\nlibrary_size: Library size of source node of shape (n_edges,)\nsrc_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ndst_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ncat_covs: Categorical covariates of source node of shape (n_edges, n_cat_covariates)</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">u</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">v</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">src_size_factor</span>,</span><span class=\"param\">\t<span class=\"n\">dst_size_factor</span>,</span><span class=\"param\">\t<span class=\"n\">src_scale</span>,</span><span class=\"param\">\t<span class=\"n\">src_bias</span>,</span><span class=\"param\">\t<span class=\"n\">src_std</span>,</span><span class=\"param\">\t<span class=\"n\">dst_scale</span>,</span><span class=\"param\">\t<span class=\"n\">dst_bias</span>,</span><span class=\"param\">\t<span class=\"n\">dst_std</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributions</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"o\">.</span><span class=\"n\">Normal</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.prob_decoders.NegativeBinomialDataDecoder", "modulename": "simba_plus.prob_decoders", "qualname": "NegativeBinomialDataDecoder", "kind": "class", "doc": "<p>Normal data decoder</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>out_features\n    Output dimensionality\nn_batches\n    Number of batches</p>\n", "bases": "ProximityDecoder"}, {"fullname": "simba_plus.prob_decoders.NegativeBinomialDataDecoder.__init__", "modulename": "simba_plus.prob_decoders", "qualname": "NegativeBinomialDataDecoder.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">out_features</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">n_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">positive_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "simba_plus.prob_decoders.NegativeBinomialDataDecoder.positive_scale", "modulename": "simba_plus.prob_decoders", "qualname": "NegativeBinomialDataDecoder.positive_scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "simba_plus.prob_decoders.NegativeBinomialDataDecoder.forward", "modulename": "simba_plus.prob_decoders", "qualname": "NegativeBinomialDataDecoder.forward", "kind": "function", "doc": "<p>Args\nu: Input source tensor of shape (n_edges, n_latent_dimension)\nv: Input destination tensor of shape (n_edges, n_latent_dimension)\nlibrary_size: Library size of source node of shape (n_edges,)\nsrc_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ndst_cont_covs: Continuous covariates of source node of shape (n_edges, n_cont_covariates)\ncat_covs: Categorical covariates of source node of shape (n_edges, n_cat_covariates)</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">u</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">v</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">src_size_factor</span>,</span><span class=\"param\">\t<span class=\"n\">dst_size_factor</span>,</span><span class=\"param\">\t<span class=\"n\">src_scale</span>,</span><span class=\"param\">\t<span class=\"n\">src_bias</span>,</span><span class=\"param\">\t<span class=\"n\">src_std</span>,</span><span class=\"param\">\t<span class=\"n\">dst_scale</span>,</span><span class=\"param\">\t<span class=\"n\">dst_bias</span>,</span><span class=\"param\">\t<span class=\"n\">dst_std</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributions</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"o\">.</span><span class=\"n\">Normal</span>:</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.readwrite", "modulename": "simba_plus.readwrite", "kind": "module", "doc": "<p>reading and writing</p>\n"}, {"fullname": "simba_plus.readwrite.read_embedding", "modulename": "simba_plus.readwrite", "qualname": "read_embedding", "kind": "function", "doc": "<p>Read in entity embeddings from pbg training</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>path_emb: <code>str</code>, optional (default: None)\n    Path to directory for pbg embedding model\n    If None, .settings.pbg_params['checkpoint_path'] will be used.\npath_entity: <code>str</code>, optional (default: None)\n    Path to entity name file\nprefix: <code>list</code>, optional (default: None)\n    A list of entity type prefixes to include.\n    By default, it reads in the embeddings of all entities.\nconvert_alias: <code>bool</code>, optional (default: True)\n    If True, it will convert entity aliases to the original indices\npath_entity_alias: <code>str</code>, optional (default: None)\n    Path to entity alias file\nnum_epochs: <code>int</code>, optional (default: None)\n    The embedding result associated with num_epochs to read in</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>dict_adata: <code>dict</code>\n    A dictionary of anndata objects of shape\n    (#entities x #dimensions)</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path_emb</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">path_entity</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">convert_alias</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">path_entity_alias</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prefix</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_epochs</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">get_marker_significance</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">path_entity_alias_marker</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.readwrite.read_10x_h5", "modulename": "simba_plus.readwrite", "qualname": "read_10x_h5", "kind": "function", "doc": "<p>Read 10x-Genomics-formatted hdf5 file.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filename\n    Path to a 10x hdf5 file.\ngenome\n    Filter expression to genes within this genome. For legacy 10x h5\n    files, this must be provided if the data contains more than one genome.\ngex_only\n    Only keep 'Gene Expression' data and ignore other feature types,\n    e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>adata: AnnData\n    Annotated data matrix, where observations/cells are named by their\n    barcode and variables/genes by gene name</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">filename</span>, </span><span class=\"param\"><span class=\"n\">genome</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">gex_only</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.readwrite.load_graph_stats", "modulename": "simba_plus.readwrite", "qualname": "load_graph_stats", "kind": "function", "doc": "<p>Load graph statistics into global setting</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>path: <code>str</code>, optional (default: None)\n    Path to the directory for graph statistics file\n    If None, <code>.settings.pbg_params['checkpoint_path']</code> will be used</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Updates <code>.settings.graph_stats</code></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.readwrite.write_bed", "modulename": "simba_plus.readwrite", "qualname": "write_bed", "kind": "function", "doc": "<p>Write peaks into .bed file</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>adata: AnnData\n    Annotated data matrix with peaks as variables.\nuse_top_pcs: <code>bool</code>, optional (default: True)\n    Use top-PCs-associated features\nfilename: <code>str</code>, optional (default: None)\n    Filename name for peaks.\n    By default, a file named 'peaks.bed' will be written to\n    <code>.settings.workdir</code></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">adata</span>, </span><span class=\"param\"><span class=\"n\">use_top_pcs</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">filename</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.simba_plus", "modulename": "simba_plus.simba_plus", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.simba_plus.logger", "modulename": "simba_plus.simba_plus", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger simba_plus.simba_plus (WARNING)&gt;"}, {"fullname": "simba_plus.simba_plus.main", "modulename": "simba_plus.simba_plus", "qualname": "main", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.train", "modulename": "simba_plus.train", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.train.logger", "modulename": "simba_plus.train", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;RootLogger root (WARNING)&gt;"}, {"fullname": "simba_plus.train.run", "modulename": "simba_plus.train", "qualname": "run", "kind": "function", "doc": "<p>Train the model with the given parameters.\nIf get_adata is True, it will only load the gene/peak/cell AnnData object from the checkpoint.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">1000000</span>,</span><span class=\"param\">\t<span class=\"n\">layers</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">n_batch_sampling</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">output_dir</span><span class=\"o\">=</span><span class=\"s1\">&#39;rna&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">data_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;../../data/atac/atac_buenrostro2018/20240808_atac_lsi_HetData.dat&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">load_checkpoint</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">reweight_rarecell</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">n_kl_warmup</span><span class=\"o\">=</span><span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">project_decoder</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">promote_indep</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_dims</span><span class=\"o\">=</span><span class=\"mi\">50</span>,</span><span class=\"param\">\t<span class=\"n\">hsic_lam</span><span class=\"o\">=</span><span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">edgetype_specific_scale</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">edgetype_specific_std</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">edgetype_specific_bias</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">nonneg</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">adata_CG</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">adata_CP</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">get_adata</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pos_scale</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.train.human_format", "modulename": "simba_plus.train", "qualname": "human_format", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">num</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.train.save_files", "modulename": "simba_plus.train", "qualname": "save_files", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">run_id</span>, </span><span class=\"param\"><span class=\"n\">cell_adata</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">peak_adata</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.train.main", "modulename": "simba_plus.train", "qualname": "main", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">args</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.train.add_argument", "modulename": "simba_plus.train", "qualname": "add_argument", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">parser</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.utils", "modulename": "simba_plus.utils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "simba_plus.utils.negative_sampling", "modulename": "simba_plus.utils", "qualname": "negative_sampling", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">edge_index</span>, </span><span class=\"param\"><span class=\"n\">num_nodes</span>, </span><span class=\"param\"><span class=\"n\">num_neg_samples_fold</span><span class=\"o\">=</span><span class=\"mi\">2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.utils.MyEarlyStopping", "modulename": "simba_plus.utils", "qualname": "MyEarlyStopping", "kind": "class", "doc": "<p>Monitor a metric and stop training when it stops improving.</p>\n\n<p>Args:\n    monitor: quantity to be monitored.\n    min_delta: minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute\n        change of less than or equal to <code>min_delta</code>, will count as no improvement.\n    patience: number of checks with no improvement\n        after which training will be stopped. Under the default configuration, one check happens after\n        every training epoch. However, the frequency of validation can be modified by setting various parameters on\n        the <code>Trainer</code>, for example <code>check_val_every_n_epoch</code> and <code>val_check_interval</code>.</p>\n\n<pre><code>    <div class=\"alert note\">\n\n    It must be noted that the patience parameter counts the number of validation checks with\n    no improvement, and not the number of training epochs. Therefore, with parameters\n    ``check_val_every_n_epoch=10`` and ``patience=3``, the trainer will perform at least 40 training\n    epochs before being stopped.\n\n    </div>\n\n\nverbose: verbosity mode.\nmode: one of ``'min'``, ``'max'``. In ``'min'`` mode, training will stop when the quantity\n    monitored has stopped decreasing and in ``'max'`` mode it will stop when the quantity\n    monitored has stopped increasing.\nstrict: whether to crash the training if `monitor` is not found in the validation metrics.\ncheck_finite: When set ``True``, stops training when the monitor becomes NaN or infinite.\nstopping_threshold: Stop training immediately once the monitored quantity reaches this threshold.\ndivergence_threshold: Stop training as soon as the monitored quantity becomes worse than this threshold.\ncheck_on_train_epoch_end: whether to run early stopping at the end of the training epoch.\n    If this is ``False``, then the check runs at the end of the validation.\nlog_rank_zero_only: When set ``True``, logs the status of the early stopping callback only for rank 0 process.\n</code></pre>\n\n<p>Raises:\n    MisconfigurationException:\n        If <code>mode</code> is none of <code>\"min\"</code> or <code>\"max\"</code>.\n    RuntimeError:\n        If the metric <code>monitor</code> is not available.</p>\n\n<p>Example::</p>\n\n<pre><code>&gt;&gt;&gt; from lightning.pytorch import Trainer\n&gt;&gt;&gt; from lightning.pytorch.callbacks import EarlyStopping\n&gt;&gt;&gt; early_stopping = EarlyStopping('val_loss')\n&gt;&gt;&gt; trainer = Trainer(callbacks=[early_stopping])\n</code></pre>\n\n<p>.. tip:: Saving and restoring multiple early stopping callbacks at the same time is supported under variation in the\n    following arguments:</p>\n\n<pre><code>*monitor, mode*\n\nRead more: :ref:`Persisting Callback State &lt;extensions/callbacks_state:save callback state&gt;`\n</code></pre>\n", "bases": "lightning.pytorch.callbacks.early_stopping.EarlyStopping"}, {"fullname": "simba_plus.utils.MyEarlyStopping.on_train_epoch_end", "modulename": "simba_plus.utils", "qualname": "MyEarlyStopping.on_train_epoch_end", "kind": "function", "doc": "<p>Called when the train epoch ends.</p>\n\n<p>To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\n<code>lightning.pytorch.core.LightningModule</code> and access them in this hook:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">MyLightningModule</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"o\">.</span><span class=\"n\">LightningModule</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">training_step_outputs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">training_step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">training_step_outputs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">loss</span>\n\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">MyCallback</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"o\">.</span><span class=\"n\">Callback</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">on_train_epoch_end</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">trainer</span><span class=\"p\">,</span> <span class=\"n\">pl_module</span><span class=\"p\">):</span>\n        <span class=\"c1\"># do something with all training_step outputs, for example:</span>\n        <span class=\"n\">epoch_mean</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">(</span><span class=\"n\">pl_module</span><span class=\"o\">.</span><span class=\"n\">training_step_outputs</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n        <span class=\"n\">pl_module</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"s2\">&quot;training_epoch_mean&quot;</span><span class=\"p\">,</span> <span class=\"n\">epoch_mean</span><span class=\"p\">)</span>\n        <span class=\"c1\"># free up the memory</span>\n        <span class=\"n\">pl_module</span><span class=\"o\">.</span><span class=\"n\">training_step_outputs</span><span class=\"o\">.</span><span class=\"n\">clear</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">trainer</span>, </span><span class=\"param\"><span class=\"n\">pl_module</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.utils.MyEarlyStopping.on_validation_end", "modulename": "simba_plus.utils", "qualname": "MyEarlyStopping.on_validation_end", "kind": "function", "doc": "<p>Called when the validation loop ends.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">trainer</span>, </span><span class=\"param\"><span class=\"n\">pl_module</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "simba_plus.utils.structured_negative_sampling", "modulename": "simba_plus.utils", "qualname": "structured_negative_sampling", "kind": "function", "doc": "<p>Samples a negative edge <code>(i,k)</code> for every positive edge\n<code>(i,j)</code> in the graph given by <code>edge_index</code>, and returns it as a\ntuple of the form <code>(i,j,k)</code>.</p>\n\n<p>Args:\n    edge_index (LongTensor): The edge indices.\n    num_nodes (int, optional): The number of nodes, <em>i.e.</em>\n        <code>max_val + 1</code> of <code>edge_index</code>. (default: <code>None</code>)\n    contains_neg_self_loops (bool, optional): If set to\n        <code>False</code>, sampled negative edges will not contain self loops.\n        (default: <code>True</code>)</p>\n\n<p>Example:</p>\n\n<pre><code>&gt;&gt;&gt; edge_index = torch.as_tensor([[0, 0, 1, 2],\n...                               [0, 1, 2, 3]])\n&gt;&gt;&gt; structured_negative_sampling(edge_index)\n(tensor([0, 0, 1, 2]), tensor([0, 1, 2, 3]), tensor([2, 3, 0, 2]))\n</code></pre>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">edge_index</span>,</span><span class=\"param\">\t<span class=\"n\">num_nodes</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">contains_neg_self_loops</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();